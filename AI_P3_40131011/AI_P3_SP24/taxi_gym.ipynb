{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_video(policy_table, name, video_width=500):\n",
    "\n",
    "    video_path = './video/' + name + '-step-0.mp4'\n",
    "        \n",
    "    env_test = gym.make('Taxi-v3', render_mode='rgb_array')\n",
    "    env_test = RecordVideo(env=env_test, video_folder=\"./video\", name_prefix=name, step_trigger=lambda x : True, disable_logger=True)\n",
    "\n",
    "    state, info = env_test.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    env_test.start_video_recorder()\n",
    "\n",
    "    while not done:\n",
    "        action = policy_table[state]\n",
    "        new_state, reward, done, truncated, info = env_test.step(int(action))\n",
    "        state = new_state\n",
    "        env_test.render()\n",
    "        steps += 1\n",
    "        if steps > 50:\n",
    "            break\n",
    "        \n",
    "    env_test.close_video_recorder()\n",
    "\n",
    "    env_test.close()\n",
    "\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Taxi Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taxi:\n",
    "    def __init__(self,\n",
    "            learning_rate=0.9, \n",
    "            discount_rate=0.8,\n",
    "            epsilon=1.0,\n",
    "            decay_rate=0.005,\n",
    "            num_iter=200,\n",
    "            num_episodes=10000,\n",
    "            max_steps=99,\n",
    "            num_evaluate_steps = 20):\n",
    "        self.env = gym.make('Taxi-v3') #initializing the environment\n",
    "        self.state_size = self.env.observation_space.n #size of state space (it is 500 in taxi environment)\n",
    "        self.action_size = self.env.action_space.n #size of action space (it is 6 in taxi environemt)\n",
    "        self.vtable = np.zeros(self.state_size) #initializing state value table\n",
    "        self.ptable = np.zeros(self.state_size) #initializing policy table (each entry contains one of 6 aciton)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size)) #initializing q value table (value for each (state, action) pair)\n",
    "        self.learning_rate = learning_rate #learning rate used in Q-leanring algorithm\n",
    "        self.discount_rate = discount_rate #discount factor (gamma)\n",
    "        self.epsilon = epsilon #epsilon for specifying randomness threshold in epsilon-greedy action used in Q-learning\n",
    "        self.decay_rate = decay_rate #decay rate for decreasing the epsilon during the algorithm\n",
    "        self.num_iter = num_iter #number of iteration used in algorihtm\n",
    "        self.num_episodes = num_episodes #number of episodes (each episode is a sequence from start to end of a single game)\n",
    "        self.max_steps = max_steps #maximum bound for number of steps in each episode\n",
    "        self.num_evaluate_steps = num_evaluate_steps #number of steps to evaluate a given policy (just used in policy iteration algorithm)\n",
    "        \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_iter=..., num_evaluate_steps=..., discount_rate=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Finding Optimal State Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use taxi.state_size and taxi.action_size\n",
    "# you can also use taxi.env.P[state][action] which returns a tuple containing (probability, next state, reward, if it is done or not)\n",
    "def value_iteration(vtable, num_iter, discount_rate):\n",
    "    for _ in range(num_iter):\n",
    "        v_old = np.copy(vtable)\n",
    "        for state in range(taxi.state_size) :\n",
    "            temp_action = np.zeros(taxi.action_size)\n",
    "            #TODO: compute the value for each action and fill the temp_action array with those values\n",
    "            vtable[state] = np.max(temp_action) #assigning the best action value to the state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extracting The Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy_extraction(vtable, ptable, num_iter, discount_rate):\n",
    "    value_iteration(vtable, num_iter, discount_rate)\n",
    "    for state in range(taxi.state_size) :\n",
    "        temp_action = np.zeros(taxi.action_size)\n",
    "        #TODO: compute the value for each action and fill the temp_action array with those values\n",
    "        ptable[state] = np.argmax(temp_action) #finding the best action by argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_extraction(taxi.vtable, taxi.ptable, taxi.num_iter, taxi.discount_rate)\n",
    "optimal_policy = taxi.ptable.copy()     # saving determined optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Tesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_video(taxi.ptable, 'value-iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_iter=..., num_evaluate_steps=..., discount_rate=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(num_iter, discount_rate):\n",
    "    vtable = np.zeros(taxi.state_size)\n",
    "    for _ in range(num_iter):\n",
    "        v_old = np.copy(vtable)\n",
    "        for state in range(taxi.state_size) :\n",
    "            ... #TODO: compute the value of the state according to the current policy\n",
    "    return vtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvement(ptable, num_iter, num_evaluate_steps, discount_rate):\n",
    "    for _ in range(num_iter):\n",
    "        vtable = evaluate(num_evaluate_steps, discount_rate).copy()\n",
    "        for state in range(taxi.state_size) :\n",
    "            temp_action = np.zeros(taxi.action_size)\n",
    "            #TODO: compute the value for each action and fill the temp_action array with those values\n",
    "            ptable[state] = np.argmax(temp_action) #improving the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement(taxi.ptable, taxi.num_iter, taxi.num_evaluate_steps, taxi.discount_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_video(taxi.ptable, 'policy-iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_episodes=..., max_steps=..., learning_rate=..., discount_rate=..., epsilon=..., decay_rate=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q_learing_train(qtable, num_episodes, max_steps, learning_rate, discount_rate, epsilon, decay_rate):\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = taxi.env.reset()\n",
    "        done = False\n",
    "        for s in range(max_steps):\n",
    "            #epsilon greedy\n",
    "            if random.uniform(0,1) < epsilon:\n",
    "                action = taxi.env.action_space.sample()\n",
    "            else:\n",
    "                action = ... #TODO: assign the action for greedy case\n",
    "            new_state, reward, done, truncated, info = taxi.env.step(action) #doing one step\n",
    "            qtable[state][action] = ... #TODO: update q-value (main part of Q-learning)\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        epsilon = np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Runing The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learing_train(taxi.qtable, taxi.num_episodes, taxi.max_steps, taxi.learning_rate, taxi.discount_rate, taxi.epsilon, taxi.decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Etracting The Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in range(taxi.state_size):\n",
    "    taxi.ptable[state] = np.argmax(taxi.qtable[state][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy_video(taxi.ptable, 'Q-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Direct Evaluation(Monte Carlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the policy is given and you should just determine the value of each state based on the given policy. Then you can evaluate states, based on the optimal policy (you found in the previous sections) and then compare the 2 matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize the parameters by filling the blanks\n",
    "taxi = Taxi(num_episodes=..., max_steps=..., discount_rate=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(ptable, num_episodes, max_steps, discount_rate):\n",
    "    count = np.ones(taxi.state_size) # counts the number of visits to each state\n",
    "    vtable = np.zeros(taxi.state_size) # store the total return for each state\n",
    "    for _ in range(num_episodes):\n",
    "        state, info = taxi.env.reset()\n",
    "        done = False\n",
    "        trajectory = [state]\n",
    "        for s in range(max_steps):\n",
    "            new_state, reward, done, truncated, info = taxi.env.step(ptable[state])\n",
    "            trajectory.append(new_state)\n",
    "            count[new_state] += 1\n",
    "            #TODO For each state visited in the current episode, update the value function vtable using the Monte Carlo update formula\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "    vtable /= count\n",
    "    return vtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_policy = np.zeros(taxi.state_size)\n",
    "given_policy_state_values = monte_carlo(given_policy, taxi.num_episodes, taxi.max_steps, taxi.discount_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_state_values = monte_carlo(optimal_policy, taxi.num_episodes, taxi.max_steps, taxi.discount_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_policy_state_values - given_policy_state_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
